{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain import hub\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe650d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.environ[\"LANGSMITH_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa289ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "osdi25_path = \"data/osdi_atc25.json\"\n",
    "def json_to_docs(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for session in data:\n",
    "        for paper in data[session]:\n",
    "            if paper[\"abstract\"] is None:\n",
    "                continue\n",
    "            try:\n",
    "                doc = Document(\n",
    "                    page_content=paper[\"abstract\"],\n",
    "                    metadata={\n",
    "                        \"title\": paper[\"title\"],\n",
    "                        \"authors\": paper[\"authors\"],\n",
    "                        \"link\": paper[\"link\"],\n",
    "                        \"session\": session\n",
    "                    }\n",
    "                )\n",
    "                docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(paper)\n",
    "\n",
    "    \n",
    "    return docs\n",
    "\n",
    "docs = []\n",
    "docs += json_to_docs(\"data/osdi_atc25.json\")\n",
    "docs += json_to_docs(\"data/osdi24_sessions.json\")\n",
    "docs += json_to_docs(\"data/nsdi25_sessions.json\")\n",
    "docs += json_to_docs(\"data/sosp24_sessions.json\")\n",
    "docs += json_to_docs(\"data/eurosys25_sessions.json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bda05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen/qwen3-30b-a3b:free\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89950ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"Point out a list of relevant papers for LLM inference\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Give a list of relevant papers for LLM inference, LLM serving, serverless, large models.\"\n",
    "query = \"Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program.  To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with programlevel context. We propose two scheduling algorithms—for single-threaded and distributed programs—that preempt and prioritize LLM calls based on their programs’ previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15× at the same latency compared to state-of-the-art systems, such as vLLM.\"\n",
    "docs = vector_store.similarity_search(query, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "\tprint(f\"{doc.metadata['title']} ({doc.metadata['session']})\")\n",
    "\tprint(doc.page_content)\n",
    "\tprint(doc.metadata[\"link\"])\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge this repo with my conference scraper\n",
    "# TODO: Add the conferences names and dates to the metadata\n",
    "# TODO: Add more conferences (MLSys, HPCA, ISCA), now I can realistically process all of this information\n",
    "# TODO: Add separate sets of AI conferences (NeurIPS, ICML, ICLR, etc.)\n",
    "# TODO: Add additional data sources (e.g. arxiv, email feeds, blogs)\n",
    "# TODO: Experiment with better / different embedding models\n",
    "# TODO: Add a way of offline storing the embedding databases for different groups of papers / conferences\n",
    "# TODO: Run some kind of cronjob that processes data feeds and alerts me of new relevant information, storing any information that was relevant\n",
    "# TODO: Experiment with better ways of doing the actual similarity search, like what queries I should use.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
